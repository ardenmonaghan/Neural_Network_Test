{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross entropy loss version of the code\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes:\n",
    "1. For cross entropy loss we do not need to do one hot encoding for the labels\n",
    "2. We do not need to use the softmax activation function as the cross entropy loss function already includes it.\n",
    "3. \n",
    "\n",
    "Mini-batch training (e.g., batch size 64 on MNIST):\n",
    "##\n",
    "60,000 training examples / 64 ≈ 937 mini-batches\n",
    "937 weight updates per epoch\n",
    "In 10 epochs, that’s ~9,370 total updates\n",
    "Full-batch training:\n",
    "##\n",
    "60,000 training examples in one batch\n",
    "1 weight update per epoch\n",
    "In 10 epochs, that’s only 10 total updates\n",
    "Even though both are “10 epochs,” mini-batch training is effectively doing way more frequent (and often more helpful) gradient steps. That’s why it can reach high accuracy in far fewer epochs.\n",
    "\n",
    "### fewer updates per epoch = slower convergence per epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "# loading the data\n",
    "training_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "testing_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(training_data)) #The amount of samples in the dataset\n",
    "\n",
    "#Note this is a tuple\n",
    "#training_data[0]\n",
    "\n",
    "#so we can access the label by \n",
    "image, label = training_data[0]  # Sample has 28x28 pixels\n",
    "print(label) #this is the label for the data\n",
    "\n",
    "image.shape #this is the shape of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, label = training_data[0]  # Sample has 28x28 pixels\n",
    "\n",
    "# Display the image using matplotlib\n",
    "plt.imshow(image.squeeze(), cmap='gray')  # Remove single channel with squeeze()\n",
    "plt.title(f\"Digit Label: {label}\")\n",
    "plt.axis('off')  # Turn off axis\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note we are doing MLP (Multi Layer Perceptron) method and not CNN (Convolutional Neural Network)\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            #28*28 is the number of features which is all of the pixels inside of an image and this will be done for each image\n",
    "            nn.Linear(in_features=28*28, out_features=20), #outfeatures is the neurons in the next layer that we want to have.\n",
    "            nn.ReLU(), #relu is the activation function\n",
    "            nn.Linear(20, 10)\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        #take in all the x features and pass it through the layers with all the rows.\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader\n",
    "\n",
    "We can put the dataset obejcts inside of a dataloader class to specify a batch_size\n",
    "the first dimension will be equal to the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = DataLoader(training_data, batch_size=64, drop_last=True)\n",
    "test_dataloader = DataLoader(testing_data, batch_size=64, drop_last=True)\n",
    "\n",
    "\n",
    "# #Dataloader is a way to load the data in batches\n",
    "# for batch in train_dataloader:\n",
    "#     print(batch) #batches are in size of 64\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    images, labels = batch\n",
    "    print(images.shape)  # e.g., torch.Size([64, 1, 28, 28])\n",
    "    print(labels.shape)  # e.g., torch.Size([64])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingData(Dataset):\n",
    "    def __init__(self):\n",
    "        #using mean squared error loss function similar to prev implementation\n",
    "        self.lossfn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def gradient_descent(self, model, dataloader, optimizer):\n",
    "        ''' we are minimizing the loss function for each batch\n",
    "        model: the NN model that we used to make a prediction\n",
    "        optimizer: the optimizer that we used to update the weights\n",
    "        train_dataloader: the dataloader that we used to load the data\n",
    "\n",
    "        '''\n",
    "        size = len(dataloader)\n",
    "\n",
    "        # Enumeration step gets the index of the current batch (X,y) X is the tenor containing the input feature ie image and y is the label.\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            prediction = model.forward(X)\n",
    "            # print(prediction.shape)\n",
    "            # print(y.shape)\n",
    "            loss = self.lossfn(prediction, y)\n",
    "\n",
    "            #Perform the backpropagation step for each layer in the model\n",
    "            #Calculates all of the partial derivatives of the loss function with respect to the weights and biases\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            #prevents the gradient from accumulating\n",
    "            optimizer.zero_grad()\n",
    "            correct = 0\n",
    "            total = 1\n",
    "            _, predicted = torch.max(prediction.data, 1)\n",
    "            total += y.size(0)\n",
    "            correct += (predicted == y).sum().item()\n",
    "            if batch % 100 == 0:\n",
    "                loss, current = loss.item(), batch * len(X)\n",
    "                print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "                \n",
    "                \n",
    "        accuracy = 100 * correct / total \n",
    "        print(f\"Accuracy: {accuracy:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.365488  [    0/  937]\n",
      "loss: 1.527867  [ 6400/  937]\n",
      "loss: 1.182376  [12800/  937]\n",
      "loss: 0.790609  [19200/  937]\n",
      "loss: 0.552362  [25600/  937]\n",
      "loss: 0.555169  [32000/  937]\n",
      "loss: 0.441997  [38400/  937]\n",
      "loss: 0.610332  [44800/  937]\n",
      "loss: 0.550044  [51200/  937]\n",
      "loss: 0.496991  [57600/  937]\n",
      "Accuracy: 86.153846  [57600/  937]\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.411821  [    0/  937]\n",
      "loss: 0.307357  [ 6400/  937]\n",
      "loss: 0.358524  [12800/  937]\n",
      "loss: 0.417523  [19200/  937]\n",
      "loss: 0.281812  [25600/  937]\n",
      "loss: 0.377521  [32000/  937]\n",
      "loss: 0.282143  [38400/  937]\n",
      "loss: 0.444355  [44800/  937]\n",
      "loss: 0.450510  [51200/  937]\n",
      "loss: 0.449160  [57600/  937]\n",
      "Accuracy: 87.692308  [57600/  937]\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.290754  [    0/  937]\n",
      "loss: 0.250213  [ 6400/  937]\n",
      "loss: 0.283116  [12800/  937]\n",
      "loss: 0.371281  [19200/  937]\n",
      "loss: 0.232635  [25600/  937]\n",
      "loss: 0.340794  [32000/  937]\n",
      "loss: 0.252266  [38400/  937]\n",
      "loss: 0.397880  [44800/  937]\n",
      "loss: 0.413980  [51200/  937]\n",
      "loss: 0.435686  [57600/  937]\n",
      "Accuracy: 90.769231  [57600/  937]\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.236025  [    0/  937]\n",
      "loss: 0.236559  [ 6400/  937]\n",
      "loss: 0.252145  [12800/  937]\n",
      "loss: 0.354829  [19200/  937]\n",
      "loss: 0.212129  [25600/  937]\n",
      "loss: 0.320154  [32000/  937]\n",
      "loss: 0.239666  [38400/  937]\n",
      "loss: 0.376346  [44800/  937]\n",
      "loss: 0.386842  [51200/  937]\n",
      "loss: 0.425696  [57600/  937]\n",
      "Accuracy: 90.769231  [57600/  937]\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.203997  [    0/  937]\n",
      "loss: 0.231005  [ 6400/  937]\n",
      "loss: 0.236672  [12800/  937]\n",
      "loss: 0.343907  [19200/  937]\n",
      "loss: 0.202387  [25600/  937]\n",
      "loss: 0.301864  [32000/  937]\n",
      "loss: 0.230689  [38400/  937]\n",
      "loss: 0.365486  [44800/  937]\n",
      "loss: 0.363618  [51200/  937]\n",
      "loss: 0.417518  [57600/  937]\n",
      "Accuracy: 90.769231  [57600/  937]\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.182194  [    0/  937]\n",
      "loss: 0.228652  [ 6400/  937]\n",
      "loss: 0.225193  [12800/  937]\n",
      "loss: 0.334927  [19200/  937]\n",
      "loss: 0.196076  [25600/  937]\n",
      "loss: 0.293147  [32000/  937]\n",
      "loss: 0.220370  [38400/  937]\n",
      "loss: 0.358606  [44800/  937]\n",
      "loss: 0.341921  [51200/  937]\n",
      "loss: 0.408761  [57600/  937]\n",
      "Accuracy: 90.769231  [57600/  937]\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.166990  [    0/  937]\n",
      "loss: 0.227431  [ 6400/  937]\n",
      "loss: 0.216740  [12800/  937]\n",
      "loss: 0.330735  [19200/  937]\n",
      "loss: 0.190956  [25600/  937]\n",
      "loss: 0.286692  [32000/  937]\n",
      "loss: 0.214056  [38400/  937]\n",
      "loss: 0.353170  [44800/  937]\n",
      "loss: 0.329651  [51200/  937]\n",
      "loss: 0.396098  [57600/  937]\n",
      "Accuracy: 90.769231  [57600/  937]\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.154815  [    0/  937]\n",
      "loss: 0.224277  [ 6400/  937]\n",
      "loss: 0.209935  [12800/  937]\n",
      "loss: 0.328235  [19200/  937]\n",
      "loss: 0.185719  [25600/  937]\n",
      "loss: 0.277965  [32000/  937]\n",
      "loss: 0.208850  [38400/  937]\n",
      "loss: 0.350850  [44800/  937]\n",
      "loss: 0.321014  [51200/  937]\n",
      "loss: 0.380873  [57600/  937]\n",
      "Accuracy: 90.769231  [57600/  937]\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.145728  [    0/  937]\n",
      "loss: 0.221230  [ 6400/  937]\n",
      "loss: 0.203502  [12800/  937]\n",
      "loss: 0.321814  [19200/  937]\n",
      "loss: 0.183006  [25600/  937]\n",
      "loss: 0.271890  [32000/  937]\n",
      "loss: 0.203446  [38400/  937]\n",
      "loss: 0.349455  [44800/  937]\n",
      "loss: 0.310993  [51200/  937]\n",
      "loss: 0.368307  [57600/  937]\n",
      "Accuracy: 92.307692  [57600/  937]\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.138405  [    0/  937]\n",
      "loss: 0.220167  [ 6400/  937]\n",
      "loss: 0.195417  [12800/  937]\n",
      "loss: 0.313842  [19200/  937]\n",
      "loss: 0.180819  [25600/  937]\n",
      "loss: 0.265576  [32000/  937]\n",
      "loss: 0.197377  [38400/  937]\n",
      "loss: 0.344928  [44800/  937]\n",
      "loss: 0.303756  [51200/  937]\n",
      "loss: 0.358329  [57600/  937]\n",
      "Accuracy: 92.307692  [57600/  937]\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "learning_rate = 0.01\n",
    "\n",
    "model = NeuralNetwork()\n",
    "\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "TrainingData = TrainingData()\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    TrainingData.gradient_descent(model, train_dataloader, optimizer)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
