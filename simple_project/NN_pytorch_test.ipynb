{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a test of the PyTorch library for neural networks\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading in the MNIST dataset\n",
    "\n",
    "# transforming pipeline\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "# loading the data\n",
    "training_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "testing_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(training_data)) #The amount of samples in the dataset\n",
    "\n",
    "#Note this is a tuple\n",
    "#training_data[0]\n",
    "\n",
    "#so we can access the label by \n",
    "image, label = training_data[0]  # Sample has 28x28 pixels\n",
    "print(label) #this is the label for the data\n",
    "\n",
    "image.shape #this is the shape of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGZCAYAAABmNy2oAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAARZ0lEQVR4nO3df6yWdf3H8ffdOR4RgsOQHydxymQMBoZUlkwJ1GSIYYsfs2FOIHVt6ipJ16SZthIwPc6x2DzLBuvHGOXmyHRlG6AkxEKNNIzZRFuCnqEeIxLqwPX9oy/vgYDwuQUOwuOxnT88u17nug7IeXLd53hZq6qqCgCIiI909QUAcPwQBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBYosXrw4arVavnXr1i1aWlrikksuiXnz5kV7e/t+m7vuuitqtVpd51u5cmXUarVYuXJlvu/xxx+Pu+6667A/xsyZM+OjH/1oXec/2PU8/PDDR+Tj7f0x9/4cS7z392Tvt9dff/2IXScnh8auvgA+nBYtWhTDhg2L//73v9He3h6///3v45577on77rsvli5dGpdddlkee/3118fll19e13k++clPxpo1a2L48OH5vscffzwWLlxYFIaTwZ7fk72dfvrpXXQ1fFiJAnU599xz4/zzz89/njp1atxyyy0xZsyYmDJlSrz00ksxYMCAiIg488wz48wzz6zrPL169YrRo0cfkWs+0b339wTq4eUjjpizzjorWltbY9u2bdHW1pbvP9DLRzt37oxvfvOb0dLSEt27d4+xY8fGM888E4MGDYqZM2fmce99aWXmzJmxcOHCiIh9XiZ55ZVXPtC1/+1vf4tZs2bFkCFDonv37jFw4MC48sor4/nnnz/g8Tt27IjZs2dHS0tLnHbaaTFu3Lh47rnn9jtu3bp18YUvfCH69OkT3bp1i0984hPxi1/84gNdKxxNosARdcUVV0RDQ0M89dRT73vcrFmz4oEHHohZs2bFsmXLYurUqTF58uTo6Oh4390dd9wR06ZNi4iINWvW5NvHPvaxD3TdmzdvjtNPPz3mz58fv/nNb2LhwoXR2NgYF1xwQWzcuHG/4+fMmRMvv/xyPPTQQ/HQQw/F5s2b4+KLL46XX345j1mxYkVcdNFF0dHREQ8++GAsW7YsRo0aFV/60pdi8eLF73s9r7zyStRqtX0CeSiTJk2KhoaG6NOnT0yZMiVeeOGFw97CHl4+4ojq0aNH9O3bNzZv3nzQYzZs2BBLliyJb33rWzFv3ryIiBg/fnwMGDAgpk+f/r4ff/Dgwfmy1JF8WWns2LExduzY/Oddu3bF5z//+RgxYkS0tbXF/fffv8/x/fr1i0ceeSTvgMaMGRNDhgyJefPmxY9+9KOIiLjxxhtjxIgRsXz58mhs/N8ftQkTJsTWrVtjzpw5ce2118ZHPnLgv5fVarVoaGiIhoaGQ157S0tLfPvb347Ro0dHr1694vnnn4/58+fH6NGj4+mnn47zzjuvrl8TTk7uFDjiDvW/6HjyyScjIuKqq67a5/3Tpk3LL57HWmdnZ8ydOzeGDx8eTU1N0djYGE1NTfHSSy/Fiy++uN/xV1999T4viZ199tlx4YUXxooVKyLify9H/fWvf40vf/nL+fH3vF1xxRWxZcuWA96B7P3xOjs748c//vEhr/3yyy+P73//+zFp0qQYO3Zs3HTTTbFq1aqo1Wrxne98p/SXgpOcKHBEbd++Pd58880444wzDnrMm2++GRGRf+Pfo7Gxsct+Wmb27Nlxxx13xBe/+MV49NFHY+3atfHHP/4xzjvvvHj33Xf3O76lpeWA79vzub3xxhsREXHrrbfGKaecss/bjTfeGBERW7duPWqfz6BBg2LMmDHxhz/84aidgxOTl484oh577LHYtWtXXHzxxQc9Zs8X/jfeeCMGDhyY7+/s7Mwvqsfaz372s7j22mtj7ty5+7x/69at0bt37/2OP9DP/7/++uv5ufXt2zciIm6//faYMmXKAc85dOjQD3jV76+qqoO+PAUH498Yjpi///3vceutt0Zzc3N89atfPehxe167X7p06T7vf/jhh6Ozs/OQ5zn11FMjIg74N/h61Wq1/Lh7PPbYY/Haa68d8PglS5bs8zLZq6++GqtXr84YDh06NIYMGRLr16+P888//4BvPXv2PGLX/16bNm2Kp59+2o/zUsydAnV54YUX8jXy9vb2WLVqVSxatCgaGhrikUceiX79+h10O2LEiJg+fXq0trZGQ0NDXHrppfGXv/wlWltbo7m5+ZB/u/34xz8eERH33HNPTJw4MRoaGmLkyJHR1NR00M2uXbsO+F8h9+jRIyZOnBiTJk2KxYsXx7Bhw2LkyJHxzDPPxL333nvQ/76ivb09Jk+eHDfccEO88847ceedd0a3bt3i9ttvz2Pa2tpi4sSJMWHChJg5c2YMHDgw3nrrrXjxxRfj2WefjV/+8pcHvd5XX301Bg8eHDNmzDjk9xUuu+yyGDt2bIwcOTK/0fyDH/wgarVafO9733vfLeynggKLFi2qIiLfmpqaqv79+1fjxo2r5s6dW7W3t++3ufPOO6v3/qu2Y8eOavbs2VX//v2rbt26VaNHj67WrFlTNTc3V7fcckset2LFiioiqhUrVuT7du7cWV1//fVVv379qlqtVkVEtWnTpoNe84wZM/a55r3fzj777Kqqqurtt9+urrvuuqp///5V9+7dqzFjxlSrVq2qxo0bV40bN26/6/npT39afe1rX6v69etXnXrqqdVnP/vZat26dfude/369dVVV11V9e/fvzrllFOqlpaW6tJLL60efPDB9/0cN23aVEVENWPGjIN+Xnt84xvfqIYPH1717NmzamxsrM4444zqmmuuqTZu3HjILbxXraoO8aMicIysXr06Lrroovj5z38eV199dVdfDpyURIEu8bvf/S7WrFkTn/rUp+K0006L9evXx/z586O5uTn+/Oc/R7du3br6EuGk5HsKdIlevXrFE088EQ888EBs27Yt+vbtGxMnTox58+YJAnQhdwoAJD+SCkASBQCSKACQDvsbzfX+7xQBOD4czreQ3SkAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkBq7+gLgUBoaGoo3zc3NR+FKjoybb765rl337t2LN0OHDi3e3HTTTcWb++67r3gzffr04k1ExI4dO4o38+fPL95897vfLd6cCNwpAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgeSDeCeass84q3jQ1NRVvLrzwwuLNmDFjijcREb179y7eTJ06ta5znWj+8Y9/FG8WLFhQvJk8eXLxZtu2bcWbiIj169cXb5588sm6znUycqcAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYBUq6qqOqwDa7WjfS3sZdSoUXXtli9fXrxpbm6u61wcW7t37y7efOUrXyne/Otf/yre1GPLli117d5+++3izcaNG+s614nmcL7cu1MAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSp6Qep/r06VPXbu3atcWbc845p65znWjq+bXr6Ogo3lxyySXFm4iI//znP8UbT8Blb56SCkARUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASI1dfQEc2FtvvVXX7rbbbiveTJo0qXjz3HPPFW8WLFhQvKnXn/70p+LN+PHjizfbt28v3owYMaJ4ExHx9a9/va4dlHCnAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAVKuqqjqsA2u1o30tdJFevXoVb7Zt21a8aWtrK95ERFx33XXFm2uuuaZ4s2TJkuINfJgczpd7dwoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiNXX0BdL1//vOfx+Q877zzzjE5T0TEDTfcULxZunRp8Wb37t3FGzieuVMAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQBSraqq6rAOrNWO9rVwguvRo0ddu0cffbR4M27cuOLNxIkTizdPPPFE8Qa6yuF8uXenAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGA5IF4HPcGDx5cvHn22WeLNx0dHcWbFStWFG/WrVtXvImIWLhwYfHmMP94c5LwQDwAiogCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEDyQDxOSJMnTy7eLFq0qHjTs2fP4k295syZU7z5yU9+UrzZsmVL8YYPBw/EA6CIKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJA/Eg/937rnnFm/uv//+4s3nPve54k292traijd333138ea1114r3nDseSAeAEVEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgeSAefAC9e/cu3lx55ZV1nWvRokXFm3r+3C5fvrx4M378+OINx54H4gFQRBQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJA8JRU+JHbu3Fm8aWxsLN50dnYWbyZMmFC8WblyZfGGD8ZTUgEoIgoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAKn8aVlwgho5cmTxZtq0acWbT3/608WbiPoeblePDRs2FG+eeuqpo3AldAV3CgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASB6Ix3Fv6NChxZubb765eDNlypTiTUtLS/HmWNq1a1fxZsuWLcWb3bt3F284PrlTACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBA8kA86lLPg+CmT59e17nqebjdoEGD6jrX8WzdunXFm7vvvrt486tf/ap4w4nDnQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIH4p1gBgwYULwZPnx48eaHP/xh8WbYsGHFm+Pd2rVrizf33ntvXedatmxZ8Wb37t11nYuTlzsFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgeUrqMdCnT5/iTVtbW13nGjVqVPHmnHPOqetcx7PVq1cXb1pbW4s3v/3tb4s37777bvEGjhV3CgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASCf1A/EuuOCC4s1tt91WvPnMZz5TvBk4cGDx5nj373//u67dggULijdz584t3mzfvr14AycadwoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEgn9QPxJk+efEw2x9KGDRuKN7/+9a+LN52dncWb1tbW4k1EREdHR107oJw7BQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoApFpVVdVhHVirHe1rAeAoOpwv9+4UAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIDUe7oFVVR3N6wDgOOBOAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYD0f3EYzaYF5FK7AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, label = training_data[0]  # Sample has 28x28 pixels\n",
    "\n",
    "# Display the image using matplotlib\n",
    "plt.imshow(image.squeeze(), cmap='gray')  # Remove single channel with squeeze()\n",
    "plt.title(f\"Digit Label: {label}\")\n",
    "plt.axis('off')  # Turn off axis\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Size: is the number of samples that we want to pass through the network at a time\n",
    "### Epochs: is the number of times we want to pass through the entire dataset\n",
    "### Learning Rate: is the step size for the gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note we are doing MLP (Multi Layer Perceptron) method and not CNN (Convolutional Neural Network)\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            #28*28 is the number of features which is all of the pixels inside of an image and this will be done for each image\n",
    "            nn.Linear(in_features=28*28, out_features=20), #outfeatures is the neurons in the next layer that we want to have.\n",
    "            nn.ReLU(), #relu is the activation function\n",
    "            nn.Linear(20, 10),\n",
    "            nn.Softmax(dim=1) #softmax to get the probability of each class between 0 and 1\n",
    "        )\n",
    "        # # Initialize weights after defining layers\n",
    "        # # Get the first and second linear layers from Sequential\n",
    "        # self.fc1 = self.linear_relu_stack[0]  # First Linear layer\n",
    "        # self.fc2 = self.linear_relu_stack[2]  # Second Linear layer\n",
    "        \n",
    "        # # Initialize weights with proper scaling\n",
    "        # nn.init.normal_(self.fc1.weight, mean=0, std=1/np.sqrt(784))\n",
    "        # nn.init.normal_(self.fc2.weight, mean=0, std=1/np.sqrt(20))\n",
    "        \n",
    "        # # Zero initialization for biases\n",
    "        # nn.init.zeros_(self.fc1.bias)\n",
    "        # nn.init.zeros_(self.fc2.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #take in all the x features and pass it through the layers with all the rows.\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "937"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Note we have training_data and test_data, we will drop the last batch as it will mess with the encoding dimensions.\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, drop_last=True)\n",
    "test_dataloader = DataLoader(testing_data, batch_size=64, drop_last=True)\n",
    "\n",
    "\n",
    "# #Dataloader is a way to load the data in batches\n",
    "# for batch in train_dataloader:\n",
    "#     print(batch) #batches are in size of 64\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    images, labels = batch\n",
    "    print(images.shape)  # e.g., torch.Size([64, 1, 28, 28])\n",
    "    print(labels.shape)  # e.g., torch.Size([64])\n",
    "    break\n",
    "\n",
    "\n",
    "#[64, 1, 28, 28]\n",
    "#28x28 is the width and height\n",
    "# 1 is the channel\n",
    "#64 is the batch size, so we converted the data into 64 sized batches\n",
    "\n",
    "#remember we ave batches and label\n",
    "len(train_dataloader) #938 * 64 = 60032 last batch has less than 64 938 batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingData(Dataset):\n",
    "    def __init__(self):\n",
    "        #using mean squared error loss function similar to prev implementation\n",
    "        self.lossfn = nn.MSELoss()\n",
    "\n",
    "    def gradient_descent(self, model, dataloader, optimizer):\n",
    "        ''' we are minimizing the loss function for each batch\n",
    "        model: the NN model that we used to make a prediction\n",
    "        optimizer: the optimizer that we used to update the weights\n",
    "        train_dataloader: the dataloader that we used to load the data\n",
    "\n",
    "        '''\n",
    "        size = len(dataloader)\n",
    "\n",
    "        # Enumeration step gets the index of the current batch (X,y) X is the tenor containing the input feature ie image and y is the label.\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            # Doing one hot encoding for the labels\n",
    "            # y_one_hot = torch.zeros(y.shape[0], 10)\n",
    "            # y_one_hot.scatter_(1, labels.unsqueeze(1).long(), 1)\n",
    "\n",
    "\n",
    "            prediction = model.forward(X)\n",
    "            loss = self.lossfn(prediction, y)\n",
    "\n",
    "            #Perform the backpropagation step for each layer in the model\n",
    "            #Calculates all of the partial derivatives of the loss function with respect to the weights and biases\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            #prevents the gradient from accumulating\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            total = 1\n",
    "            correct = 0\n",
    "            #print(y.shape)\n",
    "            _, predicted = torch.max(prediction.data, 1)\n",
    "            #print(predicted.shape, \"P\")\n",
    "            total += y.size(0)\n",
    "            correct += (predicted == y).sum().item()\n",
    "\n",
    "            if batch % 100 == 0:\n",
    "                loss, current = loss.item(), batch * len(X)\n",
    "                accuracy = 100 * correct / total\n",
    "                print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "                print(\"accuracy: \", accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/PyTorch/lib/python3.11/site-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (10) must match the size of tensor b (64) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[105], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m     \u001b[43mTrainingData\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient_descent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[104], line 23\u001b[0m, in \u001b[0;36mTrainingData.gradient_descent\u001b[0;34m(self, model, dataloader, optimizer)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch, (X, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# Doing one hot encoding for the labels\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# y_one_hot = torch.zeros(y.shape[0], 10)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# y_one_hot.scatter_(1, labels.unsqueeze(1).long(), 1)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward(X)\n\u001b[0;32m---> 23\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlossfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m#Perform the backpropagation step for each layer in the model\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m#Calculates all of the partial derivatives of the loss function with respect to the weights and biases\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/PyTorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/PyTorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/PyTorch/lib/python3.11/site-packages/torch/nn/modules/loss.py:608\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/PyTorch/lib/python3.11/site-packages/torch/nn/functional.py:3791\u001b[0m, in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3789\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3791\u001b[0m expanded_input, expanded_target \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3792\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mmse_loss(\n\u001b[1;32m   3793\u001b[0m     expanded_input, expanded_target, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[1;32m   3794\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/PyTorch/lib/python3.11/site-packages/torch/functional.py:76\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(tensors):\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[38;5;241m*\u001b[39mtensors)\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (10) must match the size of tensor b (64) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# Running the model and minizing the loss function using the gradient descent method with epochs\n",
    "epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "model = NeuralNetwork()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "TrainingData = TrainingData()\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    TrainingData.gradient_descent(model, train_dataloader, optimizer)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
