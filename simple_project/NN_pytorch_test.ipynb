{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a test of the PyTorch library for neural networks\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook is the test of the PyTorch library for neural networks using the MSE Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading in the MNIST dataset\n",
    "\n",
    "# transforming pipeline\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "# loading the data\n",
    "training_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "testing_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(training_data)) #The amount of samples in the dataset\n",
    "\n",
    "#Note this is a tuple\n",
    "#training_data[0]\n",
    "\n",
    "#so we can access the label by \n",
    "image, label = training_data[0]  # Sample has 28x28 pixels\n",
    "print(label) #this is the label for the data\n",
    "\n",
    "image.shape #this is the shape of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, label = training_data[0]  # Sample has 28x28 pixels\n",
    "\n",
    "# Display the image using matplotlib\n",
    "plt.imshow(image.squeeze(), cmap='gray')  # Remove single channel with squeeze()\n",
    "plt.title(f\"Digit Label: {label}\")\n",
    "plt.axis('off')  # Turn off axis\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Size: is the number of samples that we want to pass through the network at a time\n",
    "### Epochs: is the number of times we want to pass through the entire dataset\n",
    "### Learning Rate: is the step size for the gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note we are doing MLP (Multi Layer Perceptron) method and not CNN (Convolutional Neural Network)\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            #28*28 is the number of features which is all of the pixels inside of an image and this will be done for each image\n",
    "            nn.Linear(in_features=28*28, out_features=20), #outfeatures is the neurons in the next layer that we want to have.\n",
    "            nn.ReLU(), #relu is the activation function\n",
    "            nn.Linear(20, 10),\n",
    "            nn.Softmax(dim=1) #softmax to get the probability of each class between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        #take in all the x features and pass it through the layers with all the rows.\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "937"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Note we have training_data and test_data, we will drop the last batch as it will mess with the encoding dimensions.\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, drop_last=True)\n",
    "test_dataloader = DataLoader(testing_data, batch_size=64, drop_last=True)\n",
    "\n",
    "\n",
    "# #Dataloader is a way to load the data in batches\n",
    "# for batch in train_dataloader:\n",
    "#     print(batch) #batches are in size of 64\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    images, labels = batch\n",
    "    print(images.shape)  # e.g., torch.Size([64, 1, 28, 28])\n",
    "    print(labels.shape)  # e.g., torch.Size([64])\n",
    "\n",
    "    labels_one_hot = torch.zeros(labels.shape[0], 10)\n",
    "    labels_one_hot.scatter_(1, labels.unsqueeze(1).long(), 1)\n",
    "    print(labels_one_hot.shape)\n",
    "    break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#[64, 1, 28, 28]\n",
    "#28x28 is the width and height\n",
    "# 1 is the channel\n",
    "#64 is the batch size, so we converted the data into 64 sized batches\n",
    "\n",
    "#remember we ave batches and label\n",
    "len(train_dataloader) #938 * 64 = 60032 last batch has less than 64 938 batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes:\n",
    "\n",
    "- For batch sizes in tensors, they are the first dimension. When we grab the batch the X and y values will have the batch size as the first dimension.\n",
    "- This is noted by when we do train_dataloader and set the batch size. After iterating through a value it has the shape of [64, 1, 28, 28] and [64]\n",
    "- when we do one hot encoding technically we have a [64, 1] shape for the labels, but when we do torch.zeros(y.shape[0], 10) we are creating a tensor of shape [64, 10] where it may look like\n",
    "-[0, 0, 0, 0, 0, 0, 0, 0, 0, 0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingData(Dataset):\n",
    "    def __init__(self):\n",
    "        #using mean squared error loss function similar to prev implementation\n",
    "        self.lossfn = nn.MSELoss()\n",
    "\n",
    "    def gradient_descent(self, model, dataloader, optimizer):\n",
    "        ''' we are minimizing the loss function for each batch\n",
    "        model: the NN model that we used to make a prediction\n",
    "        optimizer: the optimizer that we used to update the weights\n",
    "        train_dataloader: the dataloader that we used to load the data\n",
    "\n",
    "        '''\n",
    "        size = len(dataloader)\n",
    "\n",
    "        # Enumeration step gets the index of the current batch (X,y) X is the tenor containing the input feature ie image and y is the label.\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            # Doing one hot encoding for the labels\n",
    "\n",
    "            #creates a 0 tensor of shape batch size x 10\n",
    "            y_one_hot = torch.zeros(y.shape[0], 10)\n",
    "            y_one_hot.scatter_(1, y.unsqueeze(1).long(), 1)\n",
    "\n",
    "\n",
    "            prediction = model.forward(X)\n",
    "\n",
    "            # print(prediction.shape)\n",
    "            loss = self.lossfn(prediction, y_one_hot)\n",
    "\n",
    "            #Perform the backpropagation step for each layer in the model\n",
    "            #Calculates all of the partial derivatives of the loss function with respect to the weights and biases\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            #prevents the gradient from accumulating\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            total = 1\n",
    "            correct = 0\n",
    "            #print(y.shape)\n",
    "            _, predicted = torch.max(prediction.data, 1)\n",
    "            #print(predicted.shape, \"P\")\n",
    "            total += y.size(0)\n",
    "            correct += (predicted == y).sum().item()\n",
    "\n",
    "            if batch % 100 == 0:\n",
    "                loss, current = loss.item(), batch * len(X)\n",
    "                accuracy = 100 * correct / total\n",
    "                print(f\"loss: {loss:>7f}  [{current:>5d}/{size*64:>5d}]\")\n",
    "                print(\"accuracy: \", accuracy)\n",
    "\n",
    "    def test_model(self, model, dataloader):\n",
    "        '''\n",
    "        model: the NN model that we used to make a prediction\n",
    "        dataloader: the dataloader that we used to load the data\n",
    "        '''\n",
    "        model.eval()\n",
    "        size = len(dataloader.dataset)\n",
    "        num_batches = len(dataloader)\n",
    "        test_loss, correct = 0, 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X, y in dataloader:\n",
    "                #one hot encoding the labels for the testing data\n",
    "                y_one_hot = torch.zeros(y.shape[0], 10)\n",
    "                y_one_hot.scatter_(1, y.unsqueeze(1).long(), 1)\n",
    "\n",
    "                pred = model.forward(X)\n",
    "\n",
    "                test_loss += self.lossfn(pred, y_one_hot).item()\n",
    "                _, pred_indices = torch.max(pred.data, dim=1)\n",
    "    \n",
    "                correct += (pred_indices == y).type(torch.float).sum().item() \n",
    "\n",
    "        test_loss /= num_batches\n",
    "        correct /= size\n",
    "        print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.090569  [    0/59968]\n",
      "accuracy:  12.307692307692308\n",
      "loss: 0.089877  [ 6400/59968]\n",
      "accuracy:  20.0\n",
      "loss: 0.089914  [12800/59968]\n",
      "accuracy:  4.615384615384615\n",
      "loss: 0.089765  [19200/59968]\n",
      "accuracy:  20.0\n",
      "loss: 0.090113  [25600/59968]\n",
      "accuracy:  12.307692307692308\n",
      "loss: 0.088821  [32000/59968]\n",
      "accuracy:  21.53846153846154\n",
      "loss: 0.086876  [38400/59968]\n",
      "accuracy:  33.84615384615385\n",
      "loss: 0.087718  [44800/59968]\n",
      "accuracy:  32.30769230769231\n",
      "loss: 0.087419  [51200/59968]\n",
      "accuracy:  26.153846153846153\n",
      "loss: 0.085743  [57600/59968]\n",
      "accuracy:  35.38461538461539\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.086245  [    0/59968]\n",
      "accuracy:  33.84615384615385\n",
      "loss: 0.085642  [ 6400/59968]\n",
      "accuracy:  35.38461538461539\n",
      "loss: 0.086136  [12800/59968]\n",
      "accuracy:  24.615384615384617\n",
      "loss: 0.085241  [19200/59968]\n",
      "accuracy:  23.076923076923077\n",
      "loss: 0.086220  [25600/59968]\n",
      "accuracy:  26.153846153846153\n",
      "loss: 0.084120  [32000/59968]\n",
      "accuracy:  33.84615384615385\n",
      "loss: 0.079798  [38400/59968]\n",
      "accuracy:  46.15384615384615\n",
      "loss: 0.083139  [44800/59968]\n",
      "accuracy:  38.46153846153846\n",
      "loss: 0.082381  [51200/59968]\n",
      "accuracy:  38.46153846153846\n",
      "loss: 0.078605  [57600/59968]\n",
      "accuracy:  44.61538461538461\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Running the model and minizing the loss function using the gradient descent method with epochs\n",
    "epochs = 10\n",
    "learning_rate = 0.01\n",
    "\n",
    "model = NeuralNetwork()\n",
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "TrainingData = TrainingData()\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    TrainingData.gradient_descent(model, train_dataloader, optimizer)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 40.8%, Avg loss: 0.080261 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Running model on testing data\n",
    "TrainingData.test_model(model, test_dataloader)\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Tensors:\n",
    "1. Batch dimension [61, 1, 28, 28] the first dimension is the batch size where 64 is the number of images in the batch.\n",
    "2. 1 is the number of channels (greyscale or RGB)\n",
    "3. 28 is the width of the image\n",
    "4. 28 is the height of the image\n",
    "\n",
    "- Labels in Pytorch are usually just [batch_size] where each entry is a single integer class ex 3 for digit 3\n",
    "\n",
    "- Flattening is turning a multi-dimensional tensor into a 1D tensor. Only affecting the row and column dimensions. \n",
    "- One hot encoding is turning a 1D tensor into a 2D tensor where the first dimension is the batch size and the second dimension is the number of classes. Usually matched to the final layer of the neural network.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
